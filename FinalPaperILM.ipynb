{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-SEA-3 final paper ILM 2016-08-22\n",
    "### can we predict healthcare spending from census data?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement and hypothesis\n",
    "\n",
    "The USA government’s expenditure on healthcare is set to reach 20 % of GDP by 2025<sup>1</sup>. There are many contributing factors, but is there some way to target efforts to reduce spending?\n",
    "\n",
    "Examining the specific geographical areas in the Centers for Medicare and Medicaid (CMS) spending data should reveal differences in spending between areas. Those differences should be able to be correlated with demographic features of those areas from US Census data.\n",
    "\n",
    "It is expected that the percentage of elderly people living in an area (75+ rears old) will be positively correlated with increased spending. Also the proportion of low income earners will also be positively correlated with increased government spending on healthcare in an area.\n",
    "\n",
    "### Description of your data set and how it was obtained\n",
    "\n",
    "The dataset is made up of three different data sources: \n",
    "1. Centers for Medicaid and Medicare outpatient spending data (2011-2014)2\n",
    "2. USA Census bureau 2010 decennial census data3\n",
    "3. US Dept of Housing and and Urban Development HUD USPS ZIP Code crosswalk Q2 20164.\n",
    "\n",
    "The CMS spending data was downloaded directly as a flat file CSV. As was the the ZIP to county crosswalk table.\n",
    "\n",
    "The 2010 Decennial census data was only available via the census API and suitable features were accessed using Laura Kurup’s Census API Python Script5. The script was configured with my census user API key, and the location_type variable was set to ‘county’. The variables requested are listed out in the census_variables.csv file. There are 78 in total covering male and female age ranges, total population, racial and ethnic demography, plus household size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORT PANDAS NUMPY MATPLOTLIB AND SEABORN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#CREATE CROSSWALK DATAFRAME\n",
    "path = '/Users/Iain/DS-SEA-3/DS-SEA-3-Project-ILM/data/HUD/'\n",
    "filename = 'zip_county_062016.csv'\n",
    "zipCountyCrosswalk = pd.read_csv(path + filename, converters={'ZIP':lambda x: str(x), 'COUNTY':lambda x:str(x)})\n",
    "\n",
    "#CREATE CMS 2014 OUTPAITENT PAYMENT INFO DATA FRAME - ENSURE LEADING ZEROS PRESENT ON ZIP CODES\n",
    "path = '/Users/Iain/DS-SEA-3/DS-SEA-3-Project-ILM/data/cms/'\n",
    "filename = 'Medicare_Provider_Charge_Outpatient_APC32_CY2014.csv'\n",
    "rawPaymentData = pd.read_csv(path + filename, converters={'Provider_Zip_Code':lambda x: str(x)})\n",
    "rawPaymentData['Provider_Zip_Code'] = rawPaymentData['Provider_Zip_Code'].apply(lambda x: x.zfill(5))\n",
    "\n",
    "#SIMPLIFY PAYMENT INFO DOWN TO SUMMED PAYMENTS PER ZIPCODE\n",
    "rawPaymentData['Total'] = rawPaymentData['Outpatient_Services'] * rawPaymentData['Average_Total_Payments'] \n",
    "paymentData = rawPaymentData[['Provider_Zip_Code','Total']].groupby(by='Provider_Zip_Code', as_index=False).sum()\n",
    "\n",
    "#CREATE CENSUSDATA DATAFRAME\n",
    "path = '/Users/Iain/DS-SEA-3/DS-SEA-3-Project-ILM/data/census/'\n",
    "filename = 'census-data-by-county-2016.08.15-03.48AM.csv'\n",
    "censusData = pd.read_csv(path + filename,   converters={'state_fips':lambda x: str(x), 'county_fips':lambda x: str(x)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and scrubbing\n",
    "\n",
    "Now create one data frame out of the three starting dataframes. To use the ```merge``` command the dataframes must share a column name which will be used to line up each row from each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CREATE FIPS CODE IN CENSUSDATA DATAFRAME\n",
    "censusData['state_fips'] = censusData['state_fips'].apply(lambda x: x.zfill(2))\n",
    "censusData['county_fips'] = censusData['county_fips'].apply(lambda x: x.zfill(3))\n",
    "censusData['FIPS'] = censusData['state_fips']+censusData['county_fips']\n",
    "\n",
    "#CHANGE COLUMS NAMES TO MERGE BY THOSE COLUMN NAMES\n",
    "zipCountyCrosswalk.rename(columns={'COUNTY':'FIPS'}, inplace=True)\n",
    "paymentData.rename(columns={'Provider_Zip_Code':'ZIP'}, inplace=True)\n",
    "\n",
    "#MERGE CENSUS AND ZIP CROSSWALK DATA FRAME\n",
    "censusZip = pd.merge(censusData, zipCountyCrosswalk, on='FIPS', how='left')\n",
    "\n",
    "#MERGE THE CENSUSZIP DATAFRAME AND THE CMS PAYMENT DATA DATAFRAME\n",
    "censusZipPayment = pd.merge(censusZip, paymentData, on='ZIP', how = 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review row counts for all the data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'censusZippayment head: ', censusZipPayment.head(50)\n",
    "print 'zipCountyCrossWalk.shape: ', zipCountyCrosswalk.shape #52314 ZIPS\n",
    "print 'paymentData.shape: ', paymentData.shape  #2806 unique ZIPs\n",
    "print 'censusData.shape: ', censusData.shape #3143 rows\n",
    "print 'censusZipPayment.shape: ', censusZipPayment.shape #52033 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to tidy up after the merge. Some Columns started off as full of Nan - those can be dropped. Also not every zip code has payment data associated with it - so those rows can be dropped. (would it be better to keep them as zeros?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ELIMINATE COLUMNS THAT ARE JUST NaN\n",
    "del censusZipPayment['Unnamed: 4']\n",
    "del censusZipPayment['Urban:_2010']\n",
    "del censusZipPayment['Urban: !! Inside urbanized areas_2010']\n",
    "del censusZipPayment['Urban: !! Inside urban clusters_2010']\n",
    "del censusZipPayment['Rural !! Inside urban clusters_2010'] \n",
    "\n",
    "#DROP ALL ROWS CONTAINING NaN\n",
    "censusZipPaymentNoNan = pd.DataFrame(censusZipPayment.dropna())\n",
    "print 'censusZipPaymentNoNan.shape: ', censusZipPaymentNoNan.shape #3877 rows left.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the rows are still of type Object instead of float64. select the columns we want to convert to float64, convert those, and then stick them back on to the non-numeric columns via concat - since the indexes should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SELECT COLUMNS TO CONVERT\n",
    "collist = ['2-person household H13. Household Size [8]_2010',\n",
    "'3-person household H13. Household Size [8]_2010',\n",
    "'4-person household H13. Household Size [8]_2010',\n",
    "'5-person household H13. Household Size [8]_2010',\n",
    "'6-person household H13. Household Size [8]_2010',\n",
    "'7-or-more-person household H13. Household Size [8]_2010',\n",
    "'Total population_2010', 'White alone_2010',\n",
    "'Black or African American alone_2010',\n",
    "'American Indian and Alaska Native alone_2010', 'Asian alone_2010',\n",
    "'Native Hawaiian and Other Pacific Islander alone_2010',\n",
    "'Some Other Race alone_2010', 'Two or More Races_2010',\n",
    "'Not Hispanic or Latino_2010', 'Hispanic or Latino_2010',\n",
    "'Total Population_2010', 'Male:_2010', 'Male: !! Under 5 years_2010',\n",
    "'Male: !! 5 to 9 years_2010', 'Male: !! 10 to 14 years_2010',\n",
    "'Male: !! 15 to 17 years_2010', 'Male: !! 18 and 19 years_2010',\n",
    "'Male: !! 20 years_2010', 'Male: !! 21 years_2010',\n",
    "'Male: !! 22 to 24 years_2010', 'Male: !! 25 to 29 years_2010',\n",
    "'Male: !! 30 to 34 years_2010', 'Male: !! 35 to 39 years_2010',\n",
    "'Male: !! 40 to 44 years_2010', 'Male: !! 45 to 49 years_2010',\n",
    "'Male: !! 50 to 54 years_2010', 'Male: !! 55 to 59 years_2010',\n",
    "'Male: !! 60 and 61 years_2010', 'Male: !! 62 to 64 years_2010',\n",
    "'Male: !! 65 and 66 years_2010', 'Male: !! 67 to 69 years_2010',\n",
    "'Male: !! 70 to 74 years_2010', 'Male: !! 75 to 79 years_2010',\n",
    "'Male: !! 80 to 84 years_2010', 'Male: !! 85 years and over_2010',\n",
    "'Female: !! 85 years and over_2010', 'Female: !! Under 5 years_2010',\n",
    "'Female: !! 5 to 9 years_2010', 'Female: !! 10 to 14 years_2010',\n",
    "'Female: !! 15 to 17 years_2010', 'Female: !! 18 and 19 years_2010',\n",
    "'Female: !! 20 years_2010', 'Female: !! 21 years_2010',\n",
    "'Female: !! 22 to 24 years_2010', 'Female: !! 25 to 29 years_2010',\n",
    "'Female: !! 30 to 34 years_2010', 'Female: !! 35 to 39 years_2010',\n",
    "'Female: !! 40 to 44 years_2010', 'Female: !! 45 to 49 years_2010',\n",
    "'Female: !! 50 to 54 years_2010', 'Female: !! 55 to 59 years_2010',\n",
    "'Female: !! 60 and 61 years_2010', 'Female: !! 62 to 64 years_2010',\n",
    "'Female: !! 65 and 66 years_2010', 'Female: !! 67 to 69 years_2010',\n",
    "'Female: !! 70 to 74 years_2010', 'Female: !! 75 to 79 years_2010',\n",
    "'Female: !! 80 to 84 years_2010','Total']\n",
    "\n",
    "#columns not to convert\n",
    "nonNumericColList = ['state', 'county', 'state_fips', 'county_fips','FIPS', 'ZIP', 'RES_RATIO', 'BUS_RATIO', 'OTH_RATIO', 'TOT_RATIO']\n",
    " \n",
    "#CREATE DATAFRAME numericChunk BY CONVERTING TO NUMERIC FROM censusZipPaymenyNoNan\n",
    "#censusZipPaymentNoNan[collist] = censusZipPaymentNoNan[collist].apply(pd.to_numeric)\n",
    "numericChunk = censusZipPaymentNoNan[collist].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "nonNumericChunk = censusZipPaymentNoNan[nonNumericColList]\n",
    "#still nulls present here 8/20 21:38!\n",
    "\n",
    "#join the chuncks back to gether on their indexes\n",
    "frames  = [nonNumericChunk,numericChunk]\n",
    "numericCensusZipPaymentNoNan = pd.concat(frames, axis=1, join = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally adjust the payment total for each zip code by the area of the county that is in the zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#res_ratio is the ratio of how much of the country is in that particualr zip code\n",
    "numericCensusZipPaymentNoNan[numericCensusZipPaymentNoNan['RES_RATIO'] != 1 ].head(5)\n",
    "\n",
    "#CREATED ADDITIONAL COLUMN TO REPRESENT PAYMENT CONTRIBUTION FROM COUNTY CENSUS DEMOGRPAHICS TAKING INTO ACCOUNT CENSUS AND COUNTY  OVERLAP\n",
    "numericCensusZipPaymentNoNan['adjTotal'] = numericCensusZipPaymentNoNan['Total'] * numericCensusZipPaymentNoNan['RES_RATIO']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data set\n",
    "\n",
    "What can we see in the data? Start by examining the distribution of payments made per zip code as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look at distribution of payment information per zip before and after adjustment for zip code county over lap\n",
    "#it's clear the adjustment pushes the distribution left and minimizes the tail.\n",
    "from scipy import stats, integrate\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "totalByZIP = numericCensusZipPaymentNoNan.groupby('ZIP').Total.sum()\n",
    "adjTotalByZIP = numericCensusZipPaymentNoNan.groupby('ZIP').adjTotal.sum()\n",
    "\n",
    "print list(adjTotalByZIP.columns)\n",
    "adjTotalByZIP.rename(columns={'','adjTotal'}, inplace=True)\n",
    "adjTotalByZIP.head(5)\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (12, 4)})\n",
    "sns.distplot(numericCensusZipPaymentNoNan['adjTotal'], bins=200, kde=False, rug=False)\n",
    "\n",
    "\n",
    "lidAdjTotalByZIP = adjTotalByZIP[ adjTotalByZIP < 250001 ]\n",
    "#sns.distplot(lidAdjTotalByZIP, bins=200, kde=False, rug=False) #ont' bother plotting with largest bar near zero\n",
    "\n",
    "#plot summed by ZIP codes data \n",
    "heelLidAdjTotalByZIP = lidAdjTotalByZIP[ lidAdjTotalByZIP > 100 ]\n",
    "sns.distplot(heelLidAdjTotalByZIP, bins=200, kde=False, rug=False)\n",
    "\n",
    "\n",
    "#sns.distplot(totalByZIP)\n",
    "#sns.distplot(adjTotalByZIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix of select census data columns vs the payment per zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "correlationcheckcollist=['2-person household H13. Household Size [8]_2010',\n",
    "'3-person household H13. Household Size [8]_2010',\n",
    "'4-person household H13. Household Size [8]_2010',\n",
    "'5-person household H13. Household Size [8]_2010',\n",
    "'6-person household H13. Household Size [8]_2010',\n",
    "'7-or-more-person household H13. Household Size [8]_2010',\n",
    "'Total population_2010', 'White alone_2010',\n",
    "'Black or African American alone_2010',\n",
    "'American Indian and Alaska Native alone_2010', 'Asian alone_2010',\n",
    "'Native Hawaiian and Other Pacific Islander alone_2010',\n",
    "'Some Other Race alone_2010', 'Two or More Races_2010',\n",
    "'Not Hispanic or Latino_2010', 'Hispanic or Latino_2010',\n",
    "'Total Population_2010', 'Male:_2010', 'Male: !! Under 5 years_2010',\n",
    "'Male: !! 5 to 9 years_2010', 'Male: !! 10 to 14 years_2010',\n",
    "'Male: !! 15 to 17 years_2010', 'Male: !! 18 and 19 years_2010',\n",
    "'Male: !! 20 years_2010', 'Male: !! 21 years_2010',\n",
    "'Male: !! 22 to 24 years_2010', 'Male: !! 25 to 29 years_2010',\n",
    "'Male: !! 30 to 34 years_2010', 'Male: !! 35 to 39 years_2010',\n",
    "'Male: !! 40 to 44 years_2010', 'Male: !! 45 to 49 years_2010',\n",
    "'Male: !! 50 to 54 years_2010', 'Male: !! 55 to 59 years_2010',\n",
    "'Male: !! 60 and 61 years_2010', 'Male: !! 62 to 64 years_2010',\n",
    "'Male: !! 65 and 66 years_2010', 'Male: !! 67 to 69 years_2010',\n",
    "'Male: !! 70 to 74 years_2010', 'Male: !! 75 to 79 years_2010',\n",
    "'Male: !! 80 to 84 years_2010', 'Male: !! 85 years and over_2010',\n",
    "'Female: !! 85 years and over_2010', 'Female: !! Under 5 years_2010',\n",
    "'Female: !! 5 to 9 years_2010', 'Female: !! 10 to 14 years_2010',\n",
    "'Female: !! 15 to 17 years_2010', 'Female: !! 18 and 19 years_2010',\n",
    "'Female: !! 20 years_2010', 'Female: !! 21 years_2010',\n",
    "'Female: !! 22 to 24 years_2010', 'Female: !! 25 to 29 years_2010',\n",
    "'Female: !! 30 to 34 years_2010', 'Female: !! 35 to 39 years_2010',\n",
    "'Female: !! 40 to 44 years_2010', 'Female: !! 45 to 49 years_2010',\n",
    "'Female: !! 50 to 54 years_2010', 'Female: !! 55 to 59 years_2010',\n",
    "'Female: !! 60 and 61 years_2010', 'Female: !! 62 to 64 years_2010',\n",
    "'Female: !! 65 and 66 years_2010', 'Female: !! 67 to 69 years_2010',\n",
    "'Female: !! 70 to 74 years_2010', 'Female: !! 75 to 79 years_2010',\n",
    "'Female: !! 80 to 84 years_2010','adjTotal']\n",
    "\n",
    "smalllist = ['adjTotal', 'Total Population_2010']\n",
    "\n",
    "pd.scatter_matrix(numericCensusZipPaymentNoNan[smalllist])\n",
    "\n",
    "pd.scatter_matrix(numericCensusZipPaymentNoNan[correlationcheckcollist])\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "#corr = numericChunk[correlationcheckcollist].corr()\n",
    "\n",
    "corr = numericCensusZipPaymentNoNan[correlationcheckcollist].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "#cmap = sns.cubehelix_palette(8, start=.5, rot=-.75, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns_plot = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3,\n",
    "           square=True, xticklabels=5, yticklabels=5,#\n",
    "          linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "fig = sns_plot.get_figure()\n",
    "fig.savefig('/Users/Iain/DS-SEA-3/DS-SEA-3-Project-ILM/viz/correlationHeatMap.png')\n",
    "\n",
    "         \n",
    "#PLOT WITH DEFAULT COLOUR MAP (BLEUGH!)       \n",
    "#sns.heatmap(corr, mask=mask, vmax=.3,\n",
    "#            square=True, xticklabels=5, yticklabels=5,\n",
    "#            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems like all the census data is not strongly correlated iwth the total payments by Zip code.\n",
    "let's test it by attempting some regressions - is anything predictive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check specified features from dataframe for non numeric values\n",
    "# ~ is negation (ie. return where False)\n",
    "#numericCensusZipPaymentNoNan[featurelist][~numericCensusZipPaymentNoNan[featurelist].applymap(np.isreal).all(1)]\n",
    "\n",
    "#numericCensusZipPaymentNoNan.loc[181] #WHY IS THIS ROW STILL HERE?!\n",
    "numericCensusZipPaymentNoNanAgain = pd.DataFrame(numericCensusZipPaymentNoNan.dropna()) #WHY DId I NEED  TO DO THIS again?!\n",
    "\n",
    "print numericCensusZipPaymentNoNanAgain.shape\n",
    "print numericCensusZipPaymentNoNan.shape\n",
    "\n",
    "#np.any(np.isnan(numericCensusZipPaymentNoNan[featurelist])) #returns True = bad\n",
    "#np.all(np.isfinite(numericCensusZipPaymentNoNan[featurelist])) #returns False = bad\n",
    "\n",
    "\n",
    "#what is the rmse we'd expect by random quessing?\n",
    "from sklearn import metrics\n",
    "numericCensusZipPaymentNoNan['nullPrediction']=numericCensusZipPaymentNoNan.adjTotal.mean()\n",
    "nullPredictionRMSE = np.sqrt(metrics.mean_squared_error(numericCensusZipPaymentNoNan.adjTotal, numericCensusZipPaymentNoNan.nullPrediction))\n",
    "#print 'nullPredictionRMSE: ', nullPredictionRMSE\n",
    "\n",
    "\n",
    "featurelist = ['Total Population_2010','White alone_2010',\n",
    "'Black or African American alone_2010',\n",
    "'American Indian and Alaska Native alone_2010', 'Asian alone_2010',\n",
    "'Native Hawaiian and Other Pacific Islander alone_2010',\n",
    "'Some Other Race alone_2010', 'Two or More Races_2010',\n",
    "'Not Hispanic or Latino_2010', 'Hispanic or Latino_2010', 'Male:_2010']\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "y = numericCensusZipPaymentNoNanAgain['adjTotal']\n",
    "X = numericCensusZipPaymentNoNanAgain[featurelist]\n",
    "\n",
    "\n",
    "linreg = LinearRegression()\n",
    "msescores = cross_val_score(linreg,X,y,cv=10,scoring='mean_squared_error')\n",
    "linregrmsescores = np.mean(np.sqrt(-msescores))\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor()\n",
    "msescores = cross_val_score(treereg,X,y,cv=10,scoring='mean_squared_error')\n",
    "treeregrmsescores = np.mean(np.sqrt(-msescores))\n",
    "\n",
    "\n",
    "print 'treereg rmsescores:', treeregrmsescores #worse than guessing\n",
    "print 'linreg rmsescores: ', linregrmsescores #only sligthly better than guessing \n",
    "print 'nullPredictionRMSE: ', nullPredictionRMSE #the guessing value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with all 75 numeric features\n",
    "1. treereg rmsescores: 2808196.86155\n",
    "2. linreg rmsescores:  2557658.1639\n",
    "3. nullPredictionRMSE:  2467141.95052\n",
    "\n",
    "WITH LIMITED FEATURES\n",
    "\n",
    "1. treereg rmsescores: 2672559.24825\n",
    "2. linreg rmsescores:  2357258.65512\n",
    "3. nullPredictionRMSE:  2467141.95052\n",
    "\n",
    "seems that the model is not very predictive usng these features.\n",
    "featurelist = ['Total Population_2010','White alone_2010',\n",
    "'Black or African American alone_2010',\n",
    "American Indian and Alaska Native alone_2010', 'Asian alone_2010',\n",
    "'Native Hawaiian and Other Pacific Islander alone_2010',\n",
    "'Some Other Race alone_2010', 'Two or More Races_2010',\n",
    "'Not Hispanic or Latino_2010', 'Hispanic or Latino_2010', 'Male:_2010']\n",
    "\n",
    "attempt some feature engineering - go in a different dirrection from census data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering - trying to improve the model\n",
    "\n",
    "what about the state a zip code is in, make dummy features for the states and attempt to fit on that basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paymentStates = pd.get_dummies(numericCensusZipPaymentNoNanAgain.state, prefix='state')\n",
    "paymentStates.drop(paymentStates.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# concatenate the original DataFrame and the dummy DataFrame\n",
    "numericCensusZipPaymentNoNanAgainStates = pd.concat([numericCensusZipPaymentNoNanAgain, paymentStates], axis=1)\n",
    "\n",
    "#set the features to be just the states - i.e. where a zip code is in at state\n",
    "featurelist = paymentStates.columns.values\n",
    "\n",
    "#set up x and y\n",
    "y = numericCensusZipPaymentNoNanAgainStates['adjTotal']\n",
    "X = numericCensusZipPaymentNoNanAgainStates[featurelist]\n",
    "\n",
    "#set up a linear regresiion estimateor and run cross validation to return rmse\n",
    "linreg = LinearRegression()\n",
    "msescores = cross_val_score(linreg,X,y,cv=10,scoring='mean_squared_error')\n",
    "linregrmsescores = np.mean(np.sqrt(-msescores))\n",
    "\n",
    "#set up a decision tree estimateor and run cross validation to return rmse\n",
    "treereg = DecisionTreeRegressor()\n",
    "msescores = cross_val_score(treereg,X,y,cv=10,scoring='mean_squared_error')\n",
    "treeregrmsescores = np.mean(np.sqrt(-msescores))\n",
    "\n",
    "#print results\n",
    "print 'treereg rmsescores:', treeregrmsescores #only sligthly better than guessing \n",
    "print 'linreg rmsescores: ', linregrmsescores #wayway worse than guessing\n",
    "print 'nullPredictionRMSE: ', nullPredictionRMSE #the guessing value. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach worked better for treereg - slightly better than random guessing, but way way worse for the linreg.\n",
    "1. treereg rmsescores: 2431290.31101\n",
    "2. linreg rmsescores:  1.04428502032e+19\n",
    "3. nullPredictionRMSE:  2467141.95052"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knnreg = KNeighboursRegressor(n_neigbors=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
